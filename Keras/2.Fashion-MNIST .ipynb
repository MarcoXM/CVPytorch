{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Fashion-MNIST\n",
    "\n",
    "[Fashion-MNIST dataset](https://github.com/zalandoresearch/fashion-mnist), Fashion-MNIST是一套28x28灰度图像的衣服。它比MNIST更复杂，因此它可以更好地表示网络的实际性能，并更好地表示您将在现实世界中使用的数据集。\n",
    "\n",
    "<img src='assets/fashion-mnist-sprite.png' width=500px>\n",
    "\n",
    "各种衣服那么多~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def imshow(image, ax=None, title=None, normalize=True):\n",
    "    fig = plt.figure(figsize = (3,3),dpi=300) \n",
    "    ax = fig.add_subplot(111)\n",
    "    if normalize:\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        image = std * image + mean\n",
    "        image = np.clip(image, 0, 1)\n",
    "\n",
    "    ax.imshow(image,cmap='gray')\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.tick_params(axis='both', length=0)\n",
    "    ax.set_xticklabels('')\n",
    "    ax.set_yticklabels('')\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see one of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 900x900 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# trainloader 是一个gennerater ，我们要读取它的信息，我们就要iter 它出来\n",
    "# 前图，后对应的label \n",
    "image, label = x_train[0],y_train[0]\n",
    "\n",
    "#使用helper的 imshow 功能读取，我们的第一张图片，这是一件xx！\n",
    "imshow(image,normalize=None);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the network\n",
    "\n",
    "Here you should define your network. As with MNIST, each image is 28x28 which is a total of 784 pixels, and there are 10 classes. You should include at least one hidden layer. We suggest you use ReLU activations for the layers and to return the logits or log-softmax from the forward pass. It's up to you how many layers you add and the size of those layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 550,346\n",
      "Trainable params: 550,346\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "\n",
    "net = Sequential()\n",
    "net.add(Flatten(input_shape = (28,28)))\n",
    "net.add(Dense(512,activation = 'relu'))\n",
    "net.add(Dense(256,activation = 'relu'))\n",
    "net.add(Dense(64,activation = 'relu'))\n",
    "net.add(Dense(10,activation = 'softmax'))\n",
    "net.summary()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the network\n",
    "\n",
    "我们把网络搭建好了，剩下的首先是要去定义我们的loss function [the criterion](http://pytorch.org/docs/master/nn.html#loss-functions) ( 我们这是一个Multi-classification Problem  `nn.CrossEntropyLoss`) and [the optimizer](http://pytorch.org/docs/master/optim.html) (优化器的意思是我们调整parameter的方式 `optim.SGD` or `optim.Adam`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.compile(loss= 'categorical_crossentropy', optimizer = 'adam',metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# one-hot encode the labels\n",
    "num_classes = len(np.unique(y_train))\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/50\n",
      " - 7s - loss: 0.5192 - acc: 0.8146 - val_loss: 0.4005 - val_acc: 0.8512\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.40053, saving model to fashionmlp.weights.best.hdf5\n",
      "Epoch 2/50\n",
      " - 6s - loss: 0.3600 - acc: 0.8684 - val_loss: 0.3578 - val_acc: 0.8698\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.40053 to 0.35780, saving model to fashionmlp.weights.best.hdf5\n",
      "Epoch 3/50\n",
      " - 7s - loss: 0.3248 - acc: 0.8802 - val_loss: 0.3430 - val_acc: 0.8772\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35780 to 0.34295, saving model to fashionmlp.weights.best.hdf5\n",
      "Epoch 4/50\n",
      " - 7s - loss: 0.2998 - acc: 0.8897 - val_loss: 0.3345 - val_acc: 0.8782\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.34295 to 0.33454, saving model to fashionmlp.weights.best.hdf5\n",
      "Epoch 5/50\n",
      " - 7s - loss: 0.2832 - acc: 0.8940 - val_loss: 0.3274 - val_acc: 0.8847\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.33454 to 0.32743, saving model to fashionmlp.weights.best.hdf5\n",
      "Epoch 6/50\n",
      " - 7s - loss: 0.2690 - acc: 0.8993 - val_loss: 0.3340 - val_acc: 0.8809\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.32743\n",
      "Epoch 7/50\n",
      " - 7s - loss: 0.2572 - acc: 0.9025 - val_loss: 0.3260 - val_acc: 0.8855\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.32743 to 0.32604, saving model to fashionmlp.weights.best.hdf5\n",
      "Epoch 8/50\n",
      " - 7s - loss: 0.2453 - acc: 0.9084 - val_loss: 0.3287 - val_acc: 0.8885\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.32604\n",
      "Epoch 9/50\n",
      " - 7s - loss: 0.2365 - acc: 0.9118 - val_loss: 0.3110 - val_acc: 0.8904\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.32604 to 0.31096, saving model to fashionmlp.weights.best.hdf5\n",
      "Epoch 10/50\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint   \n",
    "\n",
    "# train the model\n",
    "checkpointer = ModelCheckpoint(filepath='fashionmlp.weights.best.hdf5', verbose=1, \n",
    "                               save_best_only=True)\n",
    "\n",
    "hist = net.fit(x_train, y_train, batch_size=64, epochs=50,\n",
    "          validation_split=0.2, callbacks=[checkpointer], \n",
    "          verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 我们可以看到Loss 在不断地下降，我们的神经网络确实在不断地进步！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# Test out your network!\n",
    "\n",
    "y_hat = net.predict_on_batch(x_test)\n",
    "\n",
    "img = x_test[0]\n",
    "\n",
    "classes = ['T-shirt/top', 'Trouser', 'Pullover','Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag','Ankle boot']\n",
    "\n",
    "print(classes[np.argmax(y_test[0],axis=0)])\n",
    "pred = np.argmax(y_hat[0],axis=0)\n",
    "print(classes[pred])\n",
    "\n",
    "fig,(ax1,ax2) = plt.subplots(figsize = (10,5),ncols = 2)\n",
    "ax1.imshow(img,cmap='gray')\n",
    "ax1.axis('off')\n",
    "ax2.barh(np.arange(10),y_hat[0])\n",
    "ax2.set_yticks(np.arange(10))\n",
    "ax2.set_yticklabels(['T-shirt/top',\n",
    "                            'Trouser',\n",
    "                            'Pullover',\n",
    "                            'Dress',\n",
    "                            'Coat',\n",
    "                            'Sandal',\n",
    "                            'Shirt',\n",
    "                            'Sneaker',\n",
    "                            'Bag',\n",
    "                            'Ankle Boot'], size='small')\n",
    "ax2.set_title('Class Probability')\n",
    "ax2.set_xlim(0, 1.1)\n",
    "\n",
    "## Actually it is a Ankle boot!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可视化的结果如上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test 结果，我们将训练好的model 对用test 进行检测\n",
    "### 我们来看看我们这个model 有什么惊人的成绩， 经过50次迭代以后 ！\n",
    "all_pred = np.argmax(y_hat,axis=1)\n",
    "\n",
    "for images, labels in testloader: \n",
    "    # 换汤不换药 \n",
    "    output = net(images)\n",
    "\n",
    "    loss = criterion(output, labels)\n",
    "\n",
    "    test_loss += loss.item() * labels.size(0) # 这里都是算我们的loss ，补充一下，size(0)就是我们的第一个维度，batch size\n",
    "\n",
    "    \n",
    "    _, pred = torch.max(output, 1) # 我们这个max 会返回，最大值和它所在的位置\n",
    "    # pred 代表的就是哪一个node，就是我们最终选的数字是啥！\n",
    "    \n",
    "    correct = np.squeeze(pred.eq(labels.data.view_as(pred)))\n",
    "    \n",
    "    # 每一个结果，照着10个class 一个个看\n",
    "    for i in range(len(labels)): \n",
    "        label = labels.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "# 跳出循环后，算平均的loss score ，可以和我们的 training比比啊~\n",
    "test_loss = test_loss/len(testloader.sampler)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "#看每一个分类的结果!\n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            classes[i], # 对应的阿拉伯数字 \n",
    "            100 * class_correct[i] / class_total[i], # 对应的做出的选择然后是对的！\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i]))) # 打印选择的次数啥啥的 \n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total), # 这里total 就是accuracy ！！ 请问上面的是什么指标呢，每个class？\n",
    "    np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#结果可视化！\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "images.numpy()#我们要作图的话，不能直接用tensor\n",
    "\n",
    "\n",
    "\n",
    "# 使用model 做出predict\n",
    "output = net(images)\n",
    "\n",
    "_, preds_tensor = torch.max(output, 1)\n",
    "preds = preds_tensor.numpy()\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in range(20):\n",
    "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(images[idx]),cmap = 'gray')\n",
    "    ax.set_title(\"{} ({})\".format(classes[preds[idx]], classes[labels[idx]]),\n",
    "                 color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))\n",
    "    \n",
    "## 一张张打印出来，然后看看结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
